{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class Game:\n",
    "    # Actions\n",
    "    # 0 = player bergerak ke kiri\n",
    "    # 1 = player bergerak ke kanan\n",
    "    num_actions = 2\n",
    "    \n",
    "    # State\n",
    "    # 0 = player position (1 on state space)\n",
    "    # 1 = target position (2 on state space)\n",
    "    state = {\n",
    "        0 : None,\n",
    "        1 : None\n",
    "    }\n",
    "    \n",
    "    MAX_MOVEMENT = 5\n",
    "    current_movement = 5\n",
    "    \n",
    "    bot_idx = 0\n",
    "    up_idx = 9\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        player = random.randint(self.bot_idx, self.up_idx)\n",
    "        target = player\n",
    "        while target == player:\n",
    "            x = random.randint(- self.MAX_MOVEMENT, self.MAX_MOVEMENT)\n",
    "            target = (x + player if x + player >= self.bot_idx and x + player <= self.up_idx else player)\n",
    "        self.state = [player, target]\n",
    "    \n",
    "    def print_state(self):\n",
    "        print(f\"Target Position : {self.state[1]}\")\n",
    "        print(f\"Player Position : {self.state[0]}\")\n",
    "    \n",
    "    def get_state(self):\n",
    "        state = [0 for _ in range(10)]\n",
    "        state[self.state[0]] = 1 # player assign\n",
    "        state[self.state[1]] = 2 # target assign\n",
    "        return state\n",
    "    \n",
    "    def execute_action(self, action_num):\n",
    "        rewards = None\n",
    "        \n",
    "        state_before = self.state.copy()\n",
    "        if action_num == 0:\n",
    "            if self.state[0] - 1 >= self.bot_idx:\n",
    "                self.state[0] -= 1\n",
    "        else:\n",
    "            if self.state[0] + 1 <= self.up_idx:\n",
    "                self.state[0] += 1\n",
    "            \n",
    "        rewards = self.__calculate_rewards__(self.state, state_before)\n",
    "        \n",
    "        self.current_movement -= 1\n",
    "        \n",
    "        return self.get_state(), rewards, self.__check_game_end__()\n",
    "    \n",
    "    def __calculate_rewards__(self, current_state, previous_state):\n",
    "        # Calculate the distance between player and target and normalize it\n",
    "        current = abs(current_state[0] - current_state[1])\n",
    "        before = abs(previous_state[0] - previous_state[1])\n",
    "        if current < before:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def __check_game_end__(self):\n",
    "        if self.current_movement == 0:\n",
    "            return 1\n",
    "        \n",
    "        if self.state[0] == self.state[1]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 samples must be a = [state, action, rewards, next_state, is_done]\n",
    "# is_done is for determining a terminal or non-terminal state\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class ReplayMemory:\n",
    "    main_memory = []\n",
    "    max_reply = 0\n",
    "    num_batch = 0\n",
    "    def __init__(self, max_replay: int, mini_batch_num: int):\n",
    "        self.max_reply = max_replay\n",
    "        self.num_batch = mini_batch_num\n",
    "\n",
    "class DeepQAgent:\n",
    "    replay:ReplayMemory = None\n",
    "    num_actions: int = None\n",
    "    eval_model = None\n",
    "    target_model = None\n",
    "    gamma:float = None\n",
    "    epsilon:float = None\n",
    "    epsilon_min: float = None\n",
    "    epsilon_decay: float = None\n",
    "    \n",
    "    # counter for updating model weight\n",
    "    learn_counter: int = 0\n",
    "    update_weight_on: int = 0\n",
    "    \n",
    "    def __init__(self, num_actions: int, max_replay: int, mini_batch_num: int, \n",
    "                 weight_update: int, epsilon: float, epsilon_min: float, \n",
    "                 epsilon_decay:float, gamma:float):\n",
    "        self.replay = ReplayMemory(max_replay, mini_batch_num)\n",
    "        self.eval_model, self.target_model = self.create_model()\n",
    "        self.num_actions = int(num_actions)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.update_weight_on = weight_update\n",
    "        \n",
    "    def create_model(self):\n",
    "        # Create your own model and return the sequential model.\n",
    "        # Need to watchout your input is need to be a state shape\n",
    "        # And your output need to be your action shape\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(1024, input_shape=(None,10), activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(2, activation='linear'),\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.00025),\n",
    "                      loss = 'mean_squared_error',\n",
    "                      metrics= ['mse'],\n",
    "                      )\n",
    "        \n",
    "        return model, model\n",
    "        \n",
    "    def store_memory(self, state, action, rewards, next_state, is_done):\n",
    "        if len(self.replay.main_memory) == self.replay.max_reply:\n",
    "            self.replay.main_memory.pop(0)\n",
    "        self.replay.main_memory.append([state, action, rewards, \n",
    "                                        next_state, is_done])\n",
    "        \n",
    "    def pick_action(self, state, epsilon = None):\n",
    "        if epsilon == None:\n",
    "            epsilon = self.epsilon\n",
    "        action = None\n",
    "        if random.random() > epsilon:\n",
    "            if type(state) != list:\n",
    "                state = state.tolist()\n",
    "            prediction = self.eval_model.predict([state])[0]\n",
    "            action = np.argmax(prediction)\n",
    "        else:\n",
    "            action = random.randint(0, self.num_actions - 1)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.replay.main_memory) < self.replay.num_batch:\n",
    "            return\n",
    "        samples = self.__sample_mini_batch__()\n",
    "        X_current = [x[0] for x in samples]\n",
    "        X_current = np.array(X_current)\n",
    "        X_next = [x[3] for x in samples]\n",
    "        X_next = np.array(X_next)\n",
    "        \n",
    "        prediction = self.eval_model.predict(X_current)\n",
    "        target_prediction = self.target_model.predict(X_next)\n",
    "        for i in range(len(samples)):\n",
    "            if samples[i][4]: # if is_done\n",
    "                # For terminal next state\n",
    "                prediction[i][samples[i][1]] = samples[i][2]\n",
    "            else:\n",
    "                # For non-terminal next state\n",
    "                target = self.gamma * target_prediction[i][samples[i][1]]\n",
    "                prediction[i][samples[i][1]] = samples[i][2] + target\n",
    "                \n",
    "        X_train = [i[0] for i in samples]\n",
    "        X_train = np.array(X_train)\n",
    "        \n",
    "        # Tensorboard thingy\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs/\", histogram_freq=1)\n",
    "        \n",
    "        self.eval_model.fit(X_train, prediction, verbose=0, epochs=10, callbacks=[tensorboard_callback])\n",
    "        print(self.epsilon)\n",
    "        if self.learn_counter % self.update_weight_on == 0:\n",
    "            self.__update_target_models__()\n",
    "        \n",
    "        # Post Learn\n",
    "        self.learn_counter += 1\n",
    "        epsilon_after_decay = self.epsilon * self.epsilon_decay\n",
    "        if  epsilon_after_decay < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        else:\n",
    "            self.epsilon = epsilon_after_decay\n",
    "            \n",
    "    def load_model(self,path:str):\n",
    "        self.target_model = tf.keras.models.load_model(path)\n",
    "        self.eval_model = tf.keras.models.load_model(path)\n",
    "        print(\"Model Loaded\")\n",
    "        \n",
    "    def save_model(self,path:str):\n",
    "        self.eval_model.save(path)\n",
    "        print(\"Model saved\")\n",
    "            \n",
    "    def __sample_mini_batch__(self):\n",
    "        return random.sample(self.replay.main_memory, self.replay.num_batch)\n",
    "\n",
    "    def __update_target_models__(self):\n",
    "        self.target_model.set_weights(self.eval_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "%matplotlib qt\n",
    "\n",
    "class Environment:\n",
    "    game: Game\n",
    "    agent: DeepQAgent\n",
    "    step_counter: int = 1\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agent = DeepQAgent(2, 2000, 500, 5, 1, 0.001, 0.99, 0.95)\n",
    "        \n",
    "    def train(self, num_episodes: int):\n",
    "\n",
    "        # For Drawing Purposes\n",
    "        # x_axis = []\n",
    "        # y_axis = []  \n",
    "        # figure, ax = plt.subplots(figsize=(10, 8))\n",
    "        # line1, = ax.plot(x_axis, y_axis)\n",
    "        # WINDOW_RATIO = 0.1\n",
    "        # WINDOW_LIMIT = 0.85\n",
    "        # plt.title(\"Error Margin of Target and Player\", fontsize=20)\n",
    "        # plt.xlabel(\"Number of Episodes\")\n",
    "        # plt.ylabel(\"Error Margin\")\n",
    "        # figure.canvas.draw()\n",
    "        # plt.show(block=False)\n",
    "        \n",
    "        # Tensorboard\n",
    "        summary_writer1 = tf.summary.create_file_writer(\"./logs/\")\n",
    "              \n",
    "        for i in range(self.step_counter, self.step_counter+num_episodes):\n",
    "            # print(f\"Episodes {i}\")\n",
    "            self.game = Game()\n",
    "            game_end = False\n",
    "            while not game_end:\n",
    "                state = self.game.get_state()\n",
    "                action = self.agent.pick_action(state)\n",
    "                next_state, rewards, game_end = self.game.execute_action(action)\n",
    "                self.agent.store_memory(state, action, rewards,\n",
    "                                        next_state, game_end)\n",
    "            # For Model Learning Purposes\n",
    "            if i % 100 == 0:\n",
    "                self.agent.learn()\n",
    "                \n",
    "            # For Drawing of Error Margin of Target and Player\n",
    "            with summary_writer1.as_default():\n",
    "                y_temp = self.game.state\n",
    "                error_margin = ((y_temp[0] - y_temp[1])**2)**0.5\n",
    "                tf.summary.scalar(name=\"abs-error-margin\", data=error_margin, step=i)\n",
    "            summary_writer1.flush()\n",
    "            \n",
    "            # For drawing purposes\n",
    "            # x_axis.append(i)\n",
    "            # y_axis.append(((y_temp[0] - y_temp[1])**2)**0.5)\n",
    "            # line1.set_xdata(x_axis)\n",
    "            # line1.set_ydata(self.calculate_window(WINDOW_RATIO, WINDOW_LIMIT, y_axis))\n",
    "            # plt.title(f\"Error Margin of Target and Player\\nEpsilon:{self.agent.epsilon}\"\n",
    "            #           , fontsize=20)\n",
    "            \n",
    "            # ax.relim() \n",
    "            # ax.autoscale_view(True,True,True) \n",
    "\n",
    "            # figure.canvas.draw()\n",
    "            \n",
    "            # plt.pause(0.005)\n",
    "        \n",
    "        self.step_counter += num_episodes\n",
    "        \n",
    "    def play(self, num_episode: int = 100, print_state = False):\n",
    "        errors = []\n",
    "        for i in range(num_episode):\n",
    "            self.game = Game()\n",
    "            game_end = False\n",
    "            counter = 0\n",
    "            while not game_end:\n",
    "                state = self.game.get_state()\n",
    "                action = self.agent.pick_action(state)\n",
    "                _, _, game_end = self.game.execute_action(action)\n",
    "                if print_state:\n",
    "                    print(f\"Turn num {counter}\",self.game.get_state())\n",
    "                    counter += 1\n",
    "            \n",
    "            y_temp = self.game.get_state()\n",
    "            error_margin = ((y_temp[0] - y_temp[1])**2)**0.5\n",
    "            errors.append(error_margin)\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                avg_err = sum(errors) / len(errors)\n",
    "                print(f\"Current error on episode {i}: {avg_err}\")\n",
    "            \n",
    "    def calculate_window(self, win_ratio, win_limit, series):\n",
    "        copy_series = series.copy()\n",
    "        series_length = len(copy_series)\n",
    "        window_steps = int(series_length * win_ratio)\n",
    "        steps_boundary = int(series_length * win_limit)\n",
    "        if series_length < 100 + window_steps:\n",
    "            return copy_series\n",
    "        for i in range(steps_boundary):\n",
    "            copy_series[i] = np.average(copy_series[i : i+window_steps])\n",
    "        \n",
    "        return copy_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "envir = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, None, 1024)        11264     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 512)         524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 256)         131328    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, None, 128)         32896     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, None, 64)          8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, None, 2)           130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 708,674\n",
      "Trainable params: 708,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "envir.agent.eval_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 10), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 10).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 10), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 10).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 10), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 10).\n",
      "1\n",
      "0.99\n",
      "0.9801\n",
      "0.9702989999999999\n",
      "0.96059601\n",
      "0.9509900498999999\n",
      "0.9414801494009999\n",
      "0.9320653479069899\n",
      "0.92274469442792\n",
      "0.9135172474836407\n",
      "0.9043820750088043\n",
      "0.8953382542587163\n",
      "0.8863848717161291\n",
      "0.8775210229989678\n",
      "0.8687458127689781\n",
      "0.8600583546412883\n",
      "0.8514577710948754\n",
      "0.8429431933839266\n",
      "0.8345137614500874\n",
      "0.8261686238355865\n",
      "0.8179069375972307\n",
      "0.8097278682212583\n",
      "0.8016305895390458\n",
      "0.7936142836436553\n",
      "0.7856781408072188\n",
      "0.7778213593991465\n",
      "0.7700431458051551\n",
      "0.7623427143471035\n",
      "0.7547192872036325\n",
      "0.7471720943315961\n",
      "0.7397003733882802\n",
      "0.7323033696543974\n",
      "0.7249803359578534\n",
      "0.7177305325982748\n",
      "0.7105532272722921\n",
      "0.7034476949995692\n",
      "0.6964132180495735\n",
      "0.6894490858690777\n",
      "0.682554595010387\n",
      "0.6757290490602831\n",
      "0.6689717585696803\n",
      "0.6622820409839835\n",
      "0.6556592205741436\n",
      "0.6491026283684022\n",
      "0.6426116020847181\n",
      "0.6361854860638709\n",
      "0.6298236312032323\n",
      "0.6235253948912\n",
      "0.617290140942288\n",
      "0.6111172395328651\n",
      "0.6050060671375365\n",
      "0.5989560064661611\n",
      "0.5929664464014994\n",
      "0.5870367819374844\n",
      "0.5811664141181095\n",
      "0.5753547499769285\n",
      "0.5696012024771592\n",
      "0.5639051904523876\n",
      "0.5582661385478638\n",
      "0.5526834771623851\n",
      "0.5471566423907612\n",
      "0.5416850759668536\n",
      "0.536268225207185\n",
      "0.5309055429551132\n",
      "0.525596487525562\n",
      "0.5203405226503064\n",
      "0.5151371174238033\n",
      "0.5099857462495653\n",
      "0.5048858887870696\n",
      "0.4998370298991989\n",
      "0.49483865960020695\n",
      "0.4898902730042049\n",
      "0.48499137027416284\n",
      "0.4801414565714212\n",
      "0.475340042005707\n",
      "0.47058664158564995\n",
      "0.4658807751697934\n",
      "0.4612219674180955\n",
      "0.45660974774391455\n",
      "0.4520436502664754\n",
      "0.44752321376381066\n",
      "0.44304798162617254\n",
      "0.4386175018099108\n",
      "0.4342313267918117\n",
      "0.4298890135238936\n",
      "0.42559012338865465\n",
      "0.4213342221547681\n",
      "0.41712087993322045\n",
      "0.41294967113388825\n",
      "0.40882017442254937\n",
      "0.4047319726783239\n",
      "0.40068465295154065\n",
      "0.39667780642202527\n",
      "0.392711028357805\n",
      "0.38878391807422696\n",
      "0.3848960788934847\n",
      "0.38104711810454983\n",
      "0.37723664692350434\n",
      "0.37346428045426927\n",
      "0.36972963764972655\n",
      "0.36603234127322926\n",
      "0.36237201786049694\n",
      "0.358748297681892\n",
      "0.35516081470507305\n",
      "0.3516092065580223\n",
      "0.34809311449244207\n",
      "0.34461218334751764\n",
      "0.34116606151404244\n",
      "0.337754400898902\n",
      "0.334376856889913\n",
      "0.33103308832101386\n",
      "0.3277227574378037\n",
      "0.3244455298634257\n",
      "0.3212010745647914\n",
      "0.3179890638191435\n",
      "0.31480917318095203\n",
      "0.3116610814491425\n",
      "0.30854447063465107\n",
      "0.30545902592830454\n",
      "0.3024044356690215\n",
      "0.29938039131233124\n",
      "0.2963865873992079\n",
      "0.29342272152521587\n",
      "0.2904884943099637\n",
      "0.28758360936686406\n",
      "0.2847077732731954\n",
      "0.28186069554046345\n",
      "0.2790420885850588\n",
      "0.2762516676992082\n",
      "0.27348915102221616\n",
      "0.270754259511994\n",
      "0.26804671691687404\n",
      "0.2653662497477053\n",
      "0.2627125872502282\n",
      "0.2600854613777259\n",
      "0.2574846067639487\n",
      "0.2549097606963092\n",
      "0.2523606630893461\n",
      "0.24983705645845267\n",
      "0.24733868589386815\n",
      "0.24486529903492946\n",
      "0.24241664604458016\n",
      "0.23999247958413436\n",
      "0.23759255478829303\n",
      "0.2352166292404101\n",
      "0.232864462948006\n",
      "0.23053581831852593\n",
      "0.22823046013534068\n",
      "0.22594815553398728\n",
      "0.22368867397864742\n",
      "0.22145178723886094\n",
      "0.21923726936647234\n",
      "0.2170448966728076\n",
      "0.21487444770607952\n",
      "0.21272570322901874\n",
      "0.21059844619672854\n",
      "0.20849246173476127\n",
      "0.20640753711741366\n",
      "0.20434346174623952\n",
      "0.20230002712877712\n",
      "0.20027702685748935\n",
      "0.19827425658891445\n",
      "0.1962915140230253\n",
      "0.19432859888279505\n",
      "0.1923853128939671\n",
      "0.19046145976502743\n",
      "0.18855684516737714\n",
      "0.18667127671570335\n",
      "0.18480456394854633\n",
      "0.18295651830906087\n",
      "0.18112695312597027\n",
      "0.17931568359471056\n",
      "0.17752252675876345\n",
      "0.17574730149117582\n",
      "0.17398982847626407\n",
      "0.17224993019150142\n",
      "0.1705274308895864\n",
      "0.16882215658069055\n",
      "0.16713393501488363\n",
      "0.16546259566473479\n",
      "0.16380796970808745\n",
      "0.16216989001100657\n",
      "0.1605481911108965\n",
      "0.15894270919978754\n",
      "0.15735328210778965\n",
      "0.15577974928671176\n",
      "0.15422195179384465\n",
      "0.1526797322759062\n",
      "0.15115293495314713\n",
      "0.14964140560361566\n",
      "0.1481449915475795\n",
      "0.1466635416321037\n",
      "0.14519690621578268\n",
      "0.14374493715362485\n",
      "0.1423074877820886\n",
      "0.1408844129042677\n",
      "0.13947556877522502\n",
      "0.13808081308747278\n",
      "0.13670000495659804\n",
      "0.13533300490703207\n",
      "0.13397967485796175\n",
      "0.13263987810938213\n",
      "0.1313134793282883\n",
      "0.13000034453500542\n",
      "0.12870034108965536\n",
      "0.12741333767875881\n",
      "0.12613920430197123\n",
      "0.12487781225895152\n",
      "0.123629034136362\n",
      "0.12239274379499838\n",
      "0.1211688163570484\n",
      "0.11995712819347792\n",
      "0.11875755691154315\n",
      "0.11756998134242772\n",
      "0.11639428152900344\n",
      "0.11523033871371341\n",
      "0.11407803532657627\n",
      "0.11293725497331052\n",
      "0.1118078824235774\n",
      "0.11068980359934164\n",
      "0.10958290556334822\n",
      "0.10848707650771475\n",
      "0.1074022057426376\n",
      "0.10632818368521123\n",
      "0.10526490184835911\n",
      "0.10421225282987552\n",
      "0.10317013030157676\n",
      "0.10213842899856099\n",
      "0.10111704470857538\n",
      "0.10010587426148963\n",
      "0.09910481551887473\n",
      "0.09811376736368599\n",
      "0.09713262969004913\n",
      "0.09616130339314863\n",
      "0.09519969035921715\n",
      "0.09424769345562498\n",
      "0.09330521652106873\n",
      "0.09237216435585804\n",
      "0.09144844271229946\n",
      "0.09053395828517646\n",
      "0.08962861870232469\n",
      "0.08873233251530144\n",
      "0.08784500919014843\n",
      "0.08696655909824694\n",
      "0.08609689350726446\n",
      "0.08523592457219181\n",
      "0.0843835653264699\n",
      "0.0835397296732052\n",
      "0.08270433237647315\n",
      "0.08187728905270841\n",
      "0.08105851616218133\n",
      "0.08024793100055952\n",
      "0.07944545169055392\n",
      "0.07865099717364837\n",
      "0.07786448720191189\n",
      "0.07708584232989277\n",
      "0.07631498390659384\n",
      "0.07555183406752791\n",
      "0.07479631572685264\n",
      "0.07404835256958411\n",
      "0.07330786904388827\n",
      "0.07257479035344938\n",
      "0.07184904244991488\n",
      "0.07113055202541574\n",
      "0.07041924650516158\n",
      "0.06971505404010997\n",
      "0.06901790349970886\n",
      "0.06832772446471178\n",
      "0.06764444722006466\n",
      "0.066968002747864\n",
      "0.06629832272038537\n",
      "0.06563533949318151\n",
      "0.06497898609824969\n",
      "0.0643291962372672\n",
      "0.06368590427489453\n",
      "0.06304904523214558\n",
      "0.06241855477982412\n",
      "0.06179436923202588\n",
      "0.06117642553970562\n",
      "0.06056466128430856\n",
      "0.05995901467146548\n",
      "0.05935942452475082\n",
      "0.058765830279503314\n",
      "0.05817817197670828\n",
      "0.057596390256941195\n",
      "0.05702042635437178\n",
      "0.05645022209082806\n",
      "0.05588571986991978\n",
      "0.055326862671220584\n",
      "0.05477359404450838\n",
      "0.054225858104063294\n",
      "0.05368359952302266\n",
      "0.053146763527792434\n",
      "0.052615295892514506\n",
      "0.052089142933589364\n",
      "0.05156825150425347\n",
      "0.051052568989210935\n",
      "0.05054204329931883\n",
      "0.05003662286632564\n"
     ]
    }
   ],
   "source": [
    "envir.train(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# envir.agent.save_model(\"./model/own-game.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "envir = Environment()\n",
    "envir.agent.load_model(\"./model/own-game.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "139722cea5c49de78fb043fac3809b8c30f4cb03279f633027f064fb572eb5e6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
