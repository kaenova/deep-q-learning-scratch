{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Game:\n",
    "    # Actions\n",
    "    # 0 = player bergerak ke kiri\n",
    "    # 1 = player bergerak ke kanan\n",
    "    num_actions = 2\n",
    "    \n",
    "    # State\n",
    "    # 0 = player position\n",
    "    # 1 = target position\n",
    "    state = {\n",
    "        0 : None,\n",
    "        1 : None\n",
    "    }\n",
    "    \n",
    "    MAX_MOVEMENT = 5\n",
    "    current_movement = 5\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        player = random.randint(0, 5)\n",
    "        target = player\n",
    "        while target == player:\n",
    "            target = player + random.randint(-4,4)\n",
    "        self.state = [player, target]\n",
    "    \n",
    "    def print_state(self):\n",
    "        print(f\"Target Position : {self.state[1]}\")\n",
    "        print(f\"Player Position : {self.state[0]}\")\n",
    "    \n",
    "    def get_state(self):\n",
    "        return [self.state[0], self.state[1]]\n",
    "    \n",
    "    def execute_action(self, action_num):\n",
    "        rewards = None\n",
    "        \n",
    "        state_before = self.state\n",
    "        if action_num == 0:\n",
    "            self.state[0] -= 1\n",
    "        else:\n",
    "            self.state[0] += 1\n",
    "            \n",
    "        rewards = self.__calculate_rewards__(self.state, state_before)\n",
    "        \n",
    "        self.current_movement -= 1\n",
    "        \n",
    "        return self.get_state(), rewards, self.__check_game_end__()\n",
    "    \n",
    "    def __calculate_rewards__(self, current_state, previous_state):\n",
    "        # Calculate the distance between player and target and normalize it\n",
    "        delta_current = (current_state[0] - current_state[1]) ** 2\n",
    "        delta_before = (previous_state[0] - previous_state[1]) ** 2\n",
    "        \n",
    "        if delta_current < delta_before:\n",
    "            return 1 * (self.MAX_MOVEMENT - self.current_movement)\n",
    "        else:\n",
    "            return -1 * (self.MAX_MOVEMENT - self.current_movement)\n",
    "    \n",
    "    def __check_game_end__(self):\n",
    "        if self.current_movement == 0:\n",
    "            return 1\n",
    "        \n",
    "        if self.state[0] == self.state[1]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 samples must be a = [state, action, rewards, next_state, is_done]\n",
    "# is_done is for determining a terminal or non-terminal state\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class ReplayMemory:\n",
    "    main_memory = []\n",
    "    max_reply = 0\n",
    "    num_batch = 0\n",
    "    def __init__(self, max_replay: int, mini_batch_num: int):\n",
    "        self.max_reply = max_replay\n",
    "        self.num_batch = mini_batch_num\n",
    "\n",
    "class DeepQAgent:\n",
    "    replay:ReplayMemory = None\n",
    "    num_actions: int = None\n",
    "    eval_model = None\n",
    "    target_model = None\n",
    "    gamma:float = None\n",
    "    epsilon:float = None\n",
    "    epsilon_min: float = None\n",
    "    epsilon_decay: float = None\n",
    "    \n",
    "    # counter for updating model weight\n",
    "    learn_counter: int = 0\n",
    "    update_weight_on: int = 0\n",
    "    \n",
    "    def __init__(self, num_actions: int, max_replay: int, mini_batch_num: int, \n",
    "                 weight_update: int, epsilon: float, epsilon_min: float, \n",
    "                 epsilon_decay:float, gamma:float):\n",
    "        self.replay = ReplayMemory(max_replay, mini_batch_num)\n",
    "        self.eval_model, self.target_model = self.create_model()\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.update_weight_on = weight_update\n",
    "        \n",
    "    def create_model(self):\n",
    "        # Create your own model and return the sequential model.\n",
    "        # Need to watchout your input is need to be a state shape\n",
    "        # And your output need to be your action shape\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(256, input_dim=2, activation='relu'),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(2, activation='linear'),\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer = 'adam',\n",
    "                      loss = 'mean_squared_error',\n",
    "                      metrics= ['mse']\n",
    "                      )\n",
    "        \n",
    "        return model, model\n",
    "        \n",
    "    def store_memory(self, state, action, rewards, next_state, is_done):\n",
    "        if len(self.replay.main_memory) == self.replay.max_reply:\n",
    "            self.replay.main_memory.pop(0)\n",
    "        self.replay.main_memory.append([state, action, rewards, \n",
    "                                        next_state, is_done])\n",
    "        \n",
    "    def pick_action(self, state):\n",
    "        action = None\n",
    "        if random.random() < self.epsilon:\n",
    "            prediction = self.eval_model.predict([state])[0]\n",
    "            action = np.argmax(prediction)\n",
    "        else:\n",
    "            action = random.randint(0, self.num_actions - 1)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.replay.main_memory) < self.replay.num_batch:\n",
    "            return\n",
    "        \n",
    "        samples = self.__sample_mini_batch__()\n",
    "        X_samples = [x[3] for x in samples]\n",
    "        prediction = self.eval_model.predict(X_samples)\n",
    "        target_prediction = self.target_model.predict(X_samples)\n",
    "        for i in range(len(samples)):\n",
    "            if samples[i][4]: # if is_done\n",
    "                # For terminal next state\n",
    "                prediction[i][samples[i][1]] = samples[i][2]\n",
    "            else:\n",
    "                # For non-terminal next state\n",
    "                target = self.gamma * target_prediction[i][samples[i][1]]\n",
    "                prediction[i][samples[i][1]] = samples[i][2] + target\n",
    "                \n",
    "        X_train = [i[0] for i in samples]\n",
    "        X_train = np.array(X_train)\n",
    "        self.eval_model.fit(X_train, prediction, verbose=1, epochs=10)\n",
    "        if self.learn_counter % self.update_weight_on == 0:\n",
    "            self.__update_target_models__()\n",
    "        \n",
    "        # Post Learn\n",
    "        self.learn_counter += 1\n",
    "        epsilon_after_decay = self.epsilon * self.epsilon_decay\n",
    "        if  epsilon_after_decay < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        else:\n",
    "            self.epsilon = epsilon_after_decay\n",
    "            \n",
    "    def __sample_mini_batch__(self):\n",
    "        return random.sample(self.replay.main_memory, self.replay.num_batch)\n",
    "\n",
    "    def __update_target_models__(self):\n",
    "        self.target_model.set_weights(self.eval_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "%matplotlib qt\n",
    "\n",
    "class Environment:\n",
    "    game: Game\n",
    "    agent: DeepQAgent\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agent = DeepQAgent(2, 10000, 4000, 5, 1, 0.001, 0.99, 0.95)\n",
    "        \n",
    "    def train(self, num_episodes: int):\n",
    "\n",
    "        # For Drawing Purposes\n",
    "        x_axis = []\n",
    "        y_axis = []  \n",
    "        figure, ax = plt.subplots(figsize=(10, 8))\n",
    "        line1, = ax.plot(x_axis, y_axis)\n",
    "        WINDOW_RATIO = 0.1\n",
    "        WINDOW_LIMIT = 0.85\n",
    "        plt.title(\"Error Margin of Target and Player\", fontsize=20)\n",
    "        plt.xlabel(\"Number of Episodes\")\n",
    "        plt.ylabel(\"Error Margin\")\n",
    "        figure.canvas.draw()\n",
    "        plt.show(block=False)\n",
    "              \n",
    "        for i in range(1, num_episodes+1):\n",
    "            # print(f\"Episodes {i}\")\n",
    "            self.game = Game()\n",
    "            game_end = False\n",
    "            while not game_end:\n",
    "                state = self.game.get_state()\n",
    "                action = self.agent.pick_action(state)\n",
    "                next_state, rewards, game_end = self.game.execute_action(action)\n",
    "                self.agent.store_memory(state, action, rewards,\n",
    "                                        next_state, game_end)\n",
    "            \n",
    "            # For Model Learning Purposes\n",
    "            if i % 200 == 0:\n",
    "                self.agent.learn()\n",
    "                \n",
    "            # For Drawing of Error Margin of Target and Player\n",
    "            x_axis.append(i)\n",
    "            y_temp = self.game.get_state()\n",
    "            y_axis.append(((y_temp[0] - y_temp[1])**2)**0.5)\n",
    "            line1.set_xdata(x_axis)\n",
    "            line1.set_ydata(self.calculate_window(WINDOW_RATIO, WINDOW_LIMIT, y_axis))\n",
    "            plt.title(f\"Error Margin of Target and Player\\nEpsilon:{self.agent.epsilon}\"\n",
    "                      , fontsize=20)\n",
    "            \n",
    "            ax.relim() \n",
    "            ax.autoscale_view(True,True,True) \n",
    "\n",
    "            figure.canvas.draw()\n",
    "            \n",
    "            plt.pause(0.005)\n",
    "            \n",
    "    def calculate_window(self, win_ratio, win_limit, series):\n",
    "        copy_series = series.copy()\n",
    "        series_length = len(copy_series)\n",
    "        window_steps = int(series_length * win_ratio)\n",
    "        steps_boundary = int(series_length * win_limit)\n",
    "        if series_length < 100 + window_steps:\n",
    "            return copy_series\n",
    "        for i in range(steps_boundary):\n",
    "            copy_series[i] = np.average(copy_series[i : i+window_steps])\n",
    "        \n",
    "        return copy_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "envir = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.7057 - mse: 0.7057\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6307 - mse: 0.6307\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6157 - mse: 0.6157\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6013 - mse: 0.6013\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5792 - mse: 0.5792\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5793 - mse: 0.5793\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5718 - mse: 0.5718\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5684 - mse: 0.5684\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5709 - mse: 0.5709\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5605 - mse: 0.5605\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.1961 - mse: 1.1961\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.1197 - mse: 1.1197\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.0611 - mse: 1.0611\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.0310 - mse: 1.0310\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.9864 - mse: 0.9864\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.9977 - mse: 0.9977\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.9809 - mse: 0.9809\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.9727 - mse: 0.9727\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.9503 - mse: 0.9503\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.9442 - mse: 0.9442\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.3335 - mse: 1.3335\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.2767 - mse: 1.2767\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.2762 - mse: 1.2762\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.2382 - mse: 1.2382\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.2539 - mse: 1.2539\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.2637 - mse: 1.2637\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.2463 - mse: 1.2463\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.2340 - mse: 1.2340\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.2355 - mse: 1.2355\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.2254 - mse: 1.2254\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.7067 - mse: 1.7067\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6528 - mse: 1.6528\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6815 - mse: 1.6815\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6203 - mse: 1.6203\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6036 - mse: 1.6036\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5932 - mse: 1.5932\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5871 - mse: 1.5871\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5798 - mse: 1.5798\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5778 - mse: 1.5778\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6136 - mse: 1.6136\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6487 - mse: 1.6487\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5999 - mse: 1.5999\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5891 - mse: 1.5891\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5913 - mse: 1.5913\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5717 - mse: 1.5717\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5897 - mse: 1.5897\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5941 - mse: 1.5941\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5649 - mse: 1.5649\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5700 - mse: 1.5700\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5697 - mse: 1.5697\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5851 - mse: 1.5851\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5811 - mse: 1.5811\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5738 - mse: 1.5738\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5580 - mse: 1.5580\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5699 - mse: 1.5699\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5496 - mse: 1.5496\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5487 - mse: 1.5487\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5632 - mse: 1.5632\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5470 - mse: 1.5470\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5518 - mse: 1.5518\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6188 - mse: 1.6188\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6089 - mse: 1.6089\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5889 - mse: 1.5889\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6003 - mse: 1.6003\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5920 - mse: 1.5920\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5929 - mse: 1.5929\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5809 - mse: 1.5809\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5883 - mse: 1.5883\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5877 - mse: 1.5877\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.5777 - mse: 1.5777\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.7068 - mse: 1.7068\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6866 - mse: 1.6866\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6866 - mse: 1.6866\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6800 - mse: 1.6800\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6797 - mse: 1.6797\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6740 - mse: 1.6740\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6782 - mse: 1.6782\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6742 - mse: 1.6742\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6856 - mse: 1.6856\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 1.6691 - mse: 1.6691\n",
      "Epoch 1/10\n",
      " 84/125 [===================>..........] - ETA: 0s - loss: 1.6913 - mse: 1.6913"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\@Programming\\@On Going\\Data\\DeepQLearning\\DeepQ Learning Algorithm.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/DeepQ%20Learning%20Algorithm.ipynb#ch0000008?line=0'>1</a>\u001b[0m envir\u001b[39m.\u001b[39;49mtrain(\u001b[39m10000\u001b[39;49m)\n",
      "\u001b[1;32md:\\@Programming\\@On Going\\Data\\DeepQLearning\\DeepQ Learning Algorithm.ipynb Cell 3'\u001b[0m in \u001b[0;36mEnvironment.train\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/DeepQ%20Learning%20Algorithm.ipynb#ch0000004?line=37'>38</a>\u001b[0m \u001b[39m# For Model Learning Purposes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/DeepQ%20Learning%20Algorithm.ipynb#ch0000004?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m200\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/DeepQ%20Learning%20Algorithm.ipynb#ch0000004?line=39'>40</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mlearn()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/DeepQ%20Learning%20Algorithm.ipynb#ch0000004?line=41'>42</a>\u001b[0m \u001b[39m# For Drawing of Error Margin of Target and Player\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/DeepQ%20Learning%20Algorithm.ipynb#ch0000004?line=42'>43</a>\u001b[0m x_axis\u001b[39m.\u001b[39mappend(i)\n",
      "\u001b[1;32md:\\@Programming\\@On Going\\Data\\DeepQLearning\\DeepQ Learning Algorithm.ipynb Cell 2'\u001b[0m in \u001b[0;36mDeepQAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/DeepQ%20Learning%20Algorithm.ipynb#ch0000002?line=92'>93</a>\u001b[0m X_train \u001b[39m=\u001b[39m [i[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m samples]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/DeepQ%20Learning%20Algorithm.ipynb#ch0000002?line=93'>94</a>\u001b[0m X_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(X_train)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/DeepQ%20Learning%20Algorithm.ipynb#ch0000002?line=94'>95</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_model\u001b[39m.\u001b[39;49mfit(X_train, prediction, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/DeepQ%20Learning%20Algorithm.ipynb#ch0000002?line=95'>96</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn_counter \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_weight_on \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/DeepQ%20Learning%20Algorithm.ipynb#ch0000002?line=96'>97</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__update_target_models__()\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\keras\\engine\\training.py:1389\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/engine/training.py?line=1386'>1387</a>\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs  \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/engine/training.py?line=1387'>1388</a>\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/engine/training.py?line=1388'>1389</a>\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/engine/training.py?line=1389'>1390</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/engine/training.py?line=1390'>1391</a>\u001b[0m   \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=430'>431</a>\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=431'>432</a>\u001b[0m \n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=432'>433</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=433'>434</a>\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=434'>435</a>\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=435'>436</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=436'>437</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=437'>438</a>\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m'\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m'\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=294'>295</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=295'>296</a>\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=296'>297</a>\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=297'>298</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=298'>299</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=299'>300</a>\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. Expected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=314'>315</a>\u001b[0m   batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=315'>316</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=317'>318</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=319'>320</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=320'>321</a>\u001b[0m   end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=353'>354</a>\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=354'>355</a>\u001b[0m   hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=355'>356</a>\u001b[0m   hook(batch, logs)\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=357'>358</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=358'>359</a>\u001b[0m   \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=1032'>1033</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=1033'>1034</a>\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=1101'>1102</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=1103'>1104</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=1104'>1105</a>\u001b[0m   \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=1105'>1106</a>\u001b[0m   logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/callbacks.py?line=1106'>1107</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\keras\\utils\\tf_utils.py:563\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/tf_utils.py?line=559'>560</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/tf_utils.py?line=560'>561</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/tf_utils.py?line=562'>563</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/util/nest.py?line=909'>910</a>\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/util/nest.py?line=910'>911</a>\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/util/nest.py?line=912'>913</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/util/nest.py?line=913'>914</a>\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/util/nest.py?line=914'>915</a>\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/util/nest.py?line=909'>910</a>\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/util/nest.py?line=910'>911</a>\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/util/nest.py?line=912'>913</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/util/nest.py?line=913'>914</a>\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/util/nest.py?line=914'>915</a>\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\keras\\utils\\tf_utils.py:557\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/tf_utils.py?line=553'>554</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/tf_utils.py?line=554'>555</a>\u001b[0m   \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/tf_utils.py?line=555'>556</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/tf_utils.py?line=556'>557</a>\u001b[0m     t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/tf_utils.py?line=557'>558</a>\u001b[0m   \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/keras/utils/tf_utils.py?line=558'>559</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1223\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1199'>1200</a>\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1200'>1201</a>\u001b[0m \n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1201'>1202</a>\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1219'>1220</a>\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1220'>1221</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1221'>1222</a>\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1222'>1223</a>\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1223'>1224</a>\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mD:\\@Programming\\@On Going\\Data\\DeepQLearning\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1189\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1186'>1187</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1187'>1188</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1188'>1189</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1189'>1190</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/%40Programming/%40On%20Going/Data/DeepQLearning/venv/lib/site-packages/tensorflow/python/framework/ops.py?line=1190'>1191</a>\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "envir.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "139722cea5c49de78fb043fac3809b8c30f4cb03279f633027f064fb572eb5e6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
