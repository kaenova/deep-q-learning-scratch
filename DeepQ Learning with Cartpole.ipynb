{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 samples must be a = [state, action, rewards, next_state, is_done]\n",
    "# is_done is for determining a terminal or non-terminal state\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class ReplayMemory:\n",
    "    main_memory = []\n",
    "    max_reply = 0\n",
    "    num_batch = 0\n",
    "    def __init__(self, max_replay: int, mini_batch_num: int):\n",
    "        self.max_reply = max_replay\n",
    "        self.num_batch = mini_batch_num\n",
    "\n",
    "class DeepQAgent:\n",
    "    replay:ReplayMemory = None\n",
    "    num_actions: int = None\n",
    "    eval_model = None\n",
    "    target_model = None\n",
    "    gamma:float = None\n",
    "    epsilon:float = None\n",
    "    epsilon_min: float = None\n",
    "    epsilon_decay: float = None\n",
    "    \n",
    "    # counter for updating model weight\n",
    "    learn_counter: int = 0\n",
    "    update_weight_on: int = 0\n",
    "    \n",
    "    def __init__(self, num_actions: int, max_replay: int, mini_batch_num: int, \n",
    "                 weight_update: int, epsilon: float, epsilon_min: float, \n",
    "                 epsilon_decay:float, gamma:float):\n",
    "        self.replay = ReplayMemory(max_replay, mini_batch_num)\n",
    "        self.eval_model, self.target_model = self.create_model()\n",
    "        self.num_actions = int(num_actions)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.update_weight_on = weight_update\n",
    "        \n",
    "    def create_model(self):\n",
    "        # Create your own model and return the sequential model.\n",
    "        # Need to watchout your input is need to be a state shape\n",
    "        # And your output need to be your action shape\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(4,)),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(2, activation='linear'),\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer = 'adam',\n",
    "                      loss = 'mean_squared_error',\n",
    "                      metrics= ['mse']\n",
    "                      )\n",
    "        \n",
    "        return model, model\n",
    "        \n",
    "    def store_memory(self, state, action, rewards, next_state, is_done):\n",
    "        if len(self.replay.main_memory) == self.replay.max_reply:\n",
    "            self.replay.main_memory.pop(0)\n",
    "        self.replay.main_memory.append([state, action, rewards, \n",
    "                                        next_state, is_done])\n",
    "        \n",
    "    def pick_action(self, state):\n",
    "        action = None\n",
    "        if random.random() > self.epsilon:\n",
    "            if type(state) != list:\n",
    "                state = state.tolist()\n",
    "            prediction = self.eval_model.predict([state])[0]\n",
    "            action = np.argmax(prediction)\n",
    "        else:\n",
    "            action = random.randint(0, self.num_actions - 1)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.replay.main_memory) < self.replay.num_batch:\n",
    "            return\n",
    "        samples = self.__sample_mini_batch__()\n",
    "        X_current = [x[0] for x in samples]\n",
    "        X_current = np.array(X_current)\n",
    "        X_next = [x[3] for x in samples]\n",
    "        X_next = np.array(X_next)\n",
    "        prediction = self.eval_model.predict(X_current)\n",
    "        target_prediction = self.target_model.predict(X_next)\n",
    "        for i in range(len(samples)):\n",
    "            if samples[i][4]: # if is_done\n",
    "                # For terminal next state\n",
    "                prediction[i][samples[i][1]] = samples[i][2]\n",
    "            else:\n",
    "                # For non-terminal next state\n",
    "                target = self.gamma * target_prediction[i][samples[i][1]]\n",
    "                prediction[i][samples[i][1]] = samples[i][2] + target\n",
    "                \n",
    "        X_train = [i[0] for i in samples]\n",
    "        X_train = np.array(X_train)\n",
    "        self.eval_model.fit(X_train, prediction, verbose=1, epochs=10)\n",
    "        print(self.epsilon)\n",
    "        if self.learn_counter % self.update_weight_on == 0:\n",
    "            self.__update_target_models__()\n",
    "        \n",
    "        # Post Learn\n",
    "        self.learn_counter += 1\n",
    "        epsilon_after_decay = self.epsilon * self.epsilon_decay\n",
    "        if  epsilon_after_decay < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        else:\n",
    "            self.epsilon = epsilon_after_decay\n",
    "            \n",
    "    def __sample_mini_batch__(self):\n",
    "        return random.sample(self.replay.main_memory, self.replay.num_batch)\n",
    "\n",
    "    def __update_target_models__(self):\n",
    "        self.target_model.set_weights(self.eval_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "class Environment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.game = gym.make(\"CartPole-v1\")\n",
    "        action_space = self.game.action_space.n\n",
    "        print(type(action_space))\n",
    "        self.agent = DeepQAgent(action_space, 1000, 200, \n",
    "                                10, 1, 0.1, 0.995, 0.95)\n",
    "        \n",
    "    def train(self, num_ep: int):\n",
    "        for i in range(1 , num_ep):\n",
    "            print(f\"Episodes {i}\")\n",
    "            state = self.game.reset()\n",
    "            counter = 0\n",
    "            while True:\n",
    "                self.game.render()\n",
    "                action = self.agent.pick_action(state)\n",
    "                state_next, reward, terminal, info = self.game.step(action)\n",
    "                self.agent.store_memory(state, action, reward, state_next, terminal)\n",
    "                if counter % 100 == 0:\n",
    "                    self.agent.learn()\n",
    "                state = state_next\n",
    "                counter += 1\n",
    "                if terminal:\n",
    "                    break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "# env.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.3176762 , -0.2377429 ],\n",
       "       [-0.49277878, -0.36724696]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.agent.target_model(np.array([[1,2,3,4], [3,4,5,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "139722cea5c49de78fb043fac3809b8c30f4cb03279f633027f064fb572eb5e6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
